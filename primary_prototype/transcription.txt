 Jax, it's just another accelerated linear algebra library, but capable of mega fast numerical computing on futuristic new hardware. To understand Jax, let's start with the X, or accelerated linear algebra. In Python, there's already a great linear algebra library called NumPy, and Jax is nearly identical to NumPy. It allows you to create multi-dimensional rays, then do scientific computing with them, like add them together or get the dot product. However, Jax enforces some constraints that NumPy does not have, like immutable rays and pure functions, which allows it to automatically compile to low-level code that can run on accelerated hardware, like GPUs and TPUs. The A stands for Autograph. Jax was developed by Google, along with team members from the original Autograph library. You see, virtually every facet of machine learning requires some calculus. You'll need to compute gradients for optimization algorithms and back propagation and neural networks, and AutoGrad allows you to automatically differentiate Python functions, or in simple terms, it calculates the rate of change of a function based on its inputs. Then, finally, the J stands for Just in Time Compilation. When you write a function in Jax, it's transformed into a primitive set of operations. These so-called Jaxpers are lazily compiled, and can be evaluated like a many functional programming language. To get started, install Jax, either for your CPU, GPU, or TPU. Now we can start doing high-performance array computing. Let's first create a couple of two-dimensional arrays. In NumPy, if I wanted to change the values in an array, I could mutate the data structure directly, but in Jax, that's an error because arrays are immutable. Instead, we use the At function to select an index, then set to change the value, and return a new array. From there, we can do some linear algebra, like element-wise addition, element-wise multiplication, or get the dot product of the two arrays. What's really cool about Jax, though, is automatic differentiation. Imagine you're building a high-yield nuclear warhead in your mom's basement. You'll likely write a Python function that looks like this, that calculates the height of the mushroom cloud based on the amount of time after detonation. This will give us the height at any point in time, but we also want to know the instantaneous rate of change, or how fast the cloud is growing at a given point in time. With Jax, we can differentiate this function by passing our function to Jax grad, which returns a new function that computes the derivative. But if we modify this function to take an array of parameters, the end result will be a gradient, which is an array of partial derivatives. It gives us the rate of change with respect to each input variable on the original function. Because grad returns a function, we can even apply it to its own output to differentiate again and get higher order derivatives. But more importantly, you can use math just like this in Jax to build a future of machine learning. Gradients tell us how to adjust model parameters to decrease the loss or improve the performance of a model. And you can start building deep neural networks with it right now using libraries like Flax. This has been Jax in 100 seconds. But if you truly want a master machine learning, you'll also need to understand computer science, calculus, and statistics. You can start making that happen today for free thanks to this video sponsor, Brilliant. Not only does their platform have a massive collection of content related to these topics, but their fun hands on exercises will help you actually retain what you study. Just like this video, every lesson is designed to be concise and rewarding, making complex topics and math and science approachable for everyone. You can try out everything Brilliant has to offer for free for 30 days by visiting Brilliant.org slash Fireship or scan this QR code for 20% off their premium annual subscription. Thanks for watching and I will see you in the next one.